# データベース設定
database:
  path: /var/www/html/LLMKnowledge2/knowledge.db

# 基本パラメーター
parameters:
  chunk_size: 2000        # 1チャンクの最小文字数
  max_chunk_size: 3000    # 1チャンクの最大文字数
  max_retries: 5          # APIリトライ回数
  retry_delay: 5          # リトライ間隔（秒）
  api_rate_limit: 0.5     # API呼び出し間隔（秒）
  debug_mode: false       # デバッグモードの有効無効

# LLMモデル設定
models:
  openai:
    gpt-4o:
      provider: openai
      model_name: gpt-4o
      api_key: your-openai-api-key-here
    gpt-4o-mini:
      provider: openai
      model_name: gpt-4o-mini
      api_key: your-openai-api-key-here
  anthropic:
    claude-3-sonnet:
      provider: anthropic
      model_name: claude-3-5-sonnet-latest
      api_key: your-anthropic-api-key-here
    claude-3-haiku:
      provider: anthropic
      model_name: claude-3-5-haiku-latest
      api_key: your-anthropic-api-key-here
  deepseek:
    deepseek-chat:
      provider: deepseek
      model_name: deepseek-chat
      api_key: your-deepseek-api-key-here
    deepseek-reasoner:
      provider: deepseek
      model_name: deepseek-reasoner
      api_key: your-deepseek-api-key-here
  local:
    lmstudio:
      provider: openai_compatible  # OpenAI互換APIを使用
      model_name: local_model     # モデル名は任意
      endpoint: http://localhost:1234/v1  # LMStudioのデフォルトポート
      api_key: not-needed        # LMStudioではAPIキー不要
      parameters:
        temperature: 0.7
        max_tokens: 4096
        top_p: 0.95
    ollama:
      provider: ollama
      model_name: ollama
      endpoint: http://localhost:11434/api/generate
      parameters:
        temperature: 0.7
        context_length: 4096
    vllm:
      provider: openai_compatible
      model_name: vllm
      endpoint: http://localhost:8080/v1
      api_key: not-needed
      parameters:
        temperature: 0.7
        max_tokens: 4096
        top_p: 0.95

# デフォルトモデル設定
default_model: gpt-4o-mini